# imports
from keras.datasets import cifar10
from keras.models import Sequential
from keras.layers import Dense, Dropout, Flatten
from keras.layers import Convolution2D, MaxPooling2D, BatchNormalization
from keras import regularizers
from keras.utils import to_categorical
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import LearningRateScheduler
import numpy as np


# hyper-parameters
weight_decay = 0.0001
num_classes = 10
batch_size = 64
epochs = 125


# function to change learning rate
def lr_schedule(epoch):
    lrate = 0.001
    if epoch > 75:
        lrate = 0.0005
    elif epoch > 100:
        lrate = 0.0003
    return lrate

# load data
(x_train, y_train), (x_test, y_test) = cifar10.load_data()
x_train = x_train.astype('float32')
x_test = x_test.astype('float32')

# z-score = (x - x_mean)/x_std
# ie, normalize the data
mean = np.mean(x_train)
std = np.std(x_train)
x_train = (x_train-mean)/std
x_test = (x_test-mean)/std

y_train = to_categorical(y_train, num_classes)
y_test = to_categorical(y_test, num_classes)

# model
model = Sequential()
model.add(Convolution2D(32, (3, 3), input_shape=(32, 32, 3),
                        padding='same', activation='relu',
                        kernel_regularizer=regularizers.l2(weight_decay)))
model.add(BatchNormalization())
model.add(Convolution2D(32, (3, 3), padding='same', activation='relu',
                        kernel_regularizer=regularizers.l2(weight_decay)))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.2))

model.add(Convolution2D(64, (3, 3), padding='same', activation='relu',
                        kernel_regularizer=regularizers.l2(weight_decay)))
model.add(BatchNormalization())
model.add(Convolution2D(64, (3, 3), padding='same', activation='relu',
                        kernel_regularizer=regularizers.l2(weight_decay)))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.3))

model.add(Convolution2D(128, (3, 3), padding='same', activation='relu',
                        kernel_regularizer=regularizers.l2(weight_decay)))
model.add(BatchNormalization())
model.add(Convolution2D(128, (3, 3), padding='same', activation='relu',
                        kernel_regularizer=regularizers.l2(weight_decay)))
model.add(BatchNormalization())
model.add(MaxPooling2D(pool_size=(2, 2)))
model.add(Dropout(0.4))

model.add(Flatten())
model.add(Dense(num_classes, activation='softmax'))

model.summary()


# data augmentation using datagenerator
datagen = ImageDataGenerator(rotation_range=15, width_shift_range=0.1,
                             height_shift_range=0.1, horizontal_flip=True)
datagen.fit(x_train)


# training
model.compile(loss='categorical_crossentropy',
              optimizer='rmsprop', metrics=['accuracy'])
model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),
                    steps_per_epoch=x_train.shape[0]//batch_size,
                    epochs=epochs, verbose=2,
                    validation_data=(x_test, y_test),
                    callbacks=[LearningRateScheduler(lr_schedule)])


# save to disk
model_json = model.to_json()
with open('cifar10_model.json', 'w') as json_file:
    json_file.write(model_json)
model.save_weights('cifar10_model.h5')


# testing
scores = model.evaluate(x_test, y_test, batch_size=128, verbose=1)
print('\nTest result: %.3f loss: %.3f' % (scores[1]*100, scores[0]))

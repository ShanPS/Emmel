{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification with NLTK and Scikit-Learn\n",
    "** 1.IMPORTS AND LOAD DATA **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python: 3.6.1 (v3.6.1:69c0db5, Mar 21 2017, 18:41:36) [MSC v.1900 64 bit (AMD64)]\n",
      "numpy: 1.12.1\n",
      "pandas: 0.22.0\n",
      "sklearn: 0.19.1\n",
      "nltk: 3.4\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import nltk\n",
    "\n",
    "print('python: {}'.format(sys.version))\n",
    "print('numpy: {}'.format(np.__version__))\n",
    "print('pandas: {}'.format(pd.__version__))\n",
    "print('sklearn: {}'.format(sklearn.__version__))\n",
    "print('nltk: {}'.format(nltk.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "df = pd.read_table('SMSSpamCollection', encoding='utf-8', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5572 entries, 0 to 5571\n",
      "Data columns (total 2 columns):\n",
      "0    5572 non-null object\n",
      "1    5572 non-null object\n",
      "dtypes: object(2)\n",
      "memory usage: 87.1+ KB\n",
      "None\n",
      "      0                                                  1\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n"
     ]
    }
   ],
   "source": [
    "# print data details\n",
    "print(df.info())\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham     4825\n",
      "spam     747\n",
      "Name: 0, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# check data distribution\n",
    "classes = df[0]\n",
    "print(classes.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 2.PRE-PROCESS THE DATA **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     ham\n",
      "1     ham\n",
      "2    spam\n",
      "3     ham\n",
      "4     ham\n",
      "Name: 0, dtype: object\n",
      "[0 0 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "# convert labels to binary\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "Y = encoder.fit_transform(classes)\n",
    "\n",
    "print(classes[:5])\n",
    "print(Y[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Go until jurong point, crazy.. Available only ...\n",
      "1                        Ok lar... Joking wif u oni...\n",
      "2    Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3    U dun say so early hor... U c already then say...\n",
      "4    Nah I don't think he goes to usf, he lives aro...\n",
      "Name: 1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# store text messages\n",
    "texts = df[1]\n",
    "print(texts[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex to replace email-ids, urls, symbols, phone numbers, and digits\n",
    "\n",
    "processed_texts = texts.str.replace(r'^\\w+@[a-zA-Z_]+?\\.[a-zA-Z]{2,3}$', 'emailaddr')\n",
    "processed_texts = processed_texts.str.replace(r'^((http:\\/\\/www\\.)|(www\\.)|(http:\\/\\/))[a-zA-Z0-9._-]+\\.[a-zA-Z.]{2,5}$',\n",
    "                                              'webaddress')\n",
    "processed_texts = processed_texts.str.replace(r'Â£|\\$', 'moneysymb')\n",
    "processed_texts = processed_texts.str.replace(r'^\\(?[\\d]{3}\\)?[\\s-]?[\\d]{3}[\\s-]?[\\d]{4}$', 'phonenumbr')\n",
    "processed_texts = processed_texts.str.replace(r'\\d+(\\.\\d+)?', 'numbr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuations and continuous whitespaces\n",
    "processed_texts = processed_texts.str.replace(r'[^\\w\\s\\d]', ' ')\n",
    "processed_texts = processed_texts.str.replace(r'\\s+', ' ')\n",
    "processed_texts = processed_texts.str.replace(r'^\\s+|\\s+?$', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert all texts to lower case\n",
    "processed_texts = processed_texts.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    go until jurong point crazy available only in ...\n",
      "1                              ok lar joking wif u oni\n",
      "2    free entry in numbr a wkly comp to win fa cup ...\n",
      "3          u dun say so early hor u c already then say\n",
      "4    nah i don t think he goes to usf he lives arou...\n",
      "Name: 1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(processed_texts[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "processed_texts = processed_texts.apply(lambda x: ' '.join(word for word in x.split() if word not in stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do stemming using porter stemmer\n",
    "stemmer = nltk.PorterStemmer()\n",
    "\n",
    "processed_texts = processed_texts.apply(lambda x: ' '.join(stemmer.stem(word) for word in x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    go jurong point crazi avail bugi n great world...\n",
      "1                                ok lar joke wif u oni\n",
      "2    free entri numbr wkli comp win fa cup final tk...\n",
      "3                  u dun say earli hor u c alreadi say\n",
      "4                 nah think goe usf live around though\n",
      "Name: 1, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(processed_texts[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bag of words\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "all_words = []\n",
    "for text in processed_texts:\n",
    "    words = word_tokenize(text)\n",
    "    for word in words:\n",
    "        all_words.append(word)\n",
    "        \n",
    "all_words = nltk.FreqDist(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 6584\n",
      "most common 15 words: [('numbr', 2648), ('u', 1207), ('call', 674), ('go', 456), ('get', 452), ('ur', 391), ('gt', 318), ('lt', 316), ('come', 304), ('moneysymbnumbr', 303), ('ok', 293), ('free', 284), ('day', 276), ('know', 275), ('love', 266)]\n"
     ]
    }
   ],
   "source": [
    "# print number of all words and most common 15 words\n",
    "print('Vocab size: {}'.format(len(all_words)))\n",
    "print('most common 15 words: {}'.format(all_words.most_common(15)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will take most common 2000 words as features\n",
    "word_features = [word[0] for word in all_words.most_common(2000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to convert texts to feature representation\n",
    "def find_feature(message):\n",
    "    words = word_tokenize(message)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features[word] = (word in words)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make data ready to feed to the model\n",
    "messages = list(zip(processed_texts, Y))\n",
    "\n",
    "seed = 1\n",
    "np.random.seed = seed\n",
    "np.random.shuffle(messages)\n",
    "\n",
    "feature_sets = [(find_feature(text), label) for (text, label) in messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train len: 4179\n",
      "Test len: 1393\n"
     ]
    }
   ],
   "source": [
    "# split data into train and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data = train_test_split(feature_sets, test_size=0.25, random_state=seed)\n",
    "\n",
    "print('Train len: {}'.format(len(train_data)))\n",
    "print('Test len: {}'.format(len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 3.DEPLOY SKLEARN CLASSIFIERS **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import models\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('K Nearest Neighbors', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform'))\n",
      "('Decision Tree', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best'))\n",
      "('Random Forest', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
      "            oob_score=False, random_state=None, verbose=0,\n",
      "            warm_start=False))\n",
      "('Logistic Regression', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False))\n",
      "('SGD Classifier', SGDClassifier(alpha=0.0001, average=False, class_weight=None, epsilon=0.1,\n",
      "       eta0=0.0, fit_intercept=True, l1_ratio=0.15,\n",
      "       learning_rate='optimal', loss='hinge', max_iter=100, n_iter=None,\n",
      "       n_jobs=1, penalty='l2', power_t=0.5, random_state=None,\n",
      "       shuffle=True, tol=None, verbose=0, warm_start=False))\n",
      "('Naive Bayes', MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))\n",
      "('SVM Linear', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))\n"
     ]
    }
   ],
   "source": [
    "# define models to train\n",
    "names = ['K Nearest Neighbors', 'Decision Tree', 'Random Forest', 'Logistic Regression',\n",
    "         'SGD Classifier', 'Naive Bayes', 'SVM Linear']\n",
    "classifier = [KNeighborsClassifier(), DecisionTreeClassifier(), RandomForestClassifier(),\n",
    "              LogisticRegression(), SGDClassifier(max_iter=100), MultinomialNB(), SVC(kernel='linear')]\n",
    "\n",
    "models = list(zip(names, classifier))\n",
    "for model in models:\n",
    "    print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K Nearest Neighbors: Accuracy = 94.25699928212491\n",
      "Decision Tree: Accuracy = 97.4156496769562\n",
      "Random Forest: Accuracy = 97.77458722182341\n",
      "Logistic Regression: Accuracy = 98.7078248384781\n",
      "SGD Classifier: Accuracy = 98.42067480258436\n",
      "Naive Bayes: Accuracy = 98.49246231155779\n",
      "SVM Linear: Accuracy = 98.77961234745155\n"
     ]
    }
   ],
   "source": [
    "# wrap models in NLTK\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "\n",
    "for name, model in models:\n",
    "    nltk_model = SklearnClassifier(model)\n",
    "    nltk_model.train(train_data)\n",
    "    accuracy = nltk.classify.accuracy(nltk_model, test_data) * 100\n",
    "    print('{}: Accuracy = {}'.format(name, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble method: Accuracy = 98.77961234745155\n"
     ]
    }
   ],
   "source": [
    "# ensemble method - voting classifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "names = ['K Nearest Neighbors', 'Decision Tree', 'Random Forest', 'Logistic Regression',\n",
    "         'SGD Classifier', 'Naive Bayes', 'SVM Linear']\n",
    "classifier = [KNeighborsClassifier(), DecisionTreeClassifier(), RandomForestClassifier(),\n",
    "              LogisticRegression(), SGDClassifier(max_iter=100), MultinomialNB(), SVC(kernel='linear')]\n",
    "\n",
    "models = list(zip(names, classifier))\n",
    "\n",
    "nltk_ensemble = SklearnClassifier(VotingClassifier(estimators=models, voting='hard', n_jobs=-1))\n",
    "nltk_ensemble.train(train_data)\n",
    "accuracy = nltk.classify.accuracy(nltk_ensemble, test_data) * 100\n",
    "print('Ensemble method: Accuracy = {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make predictions separately\n",
    "test_texts, test_labels = zip(*test_data)\n",
    "\n",
    "predictions = nltk_ensemble.classify_many(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      0.99      1209\n",
      "          1       1.00      0.91      0.95       184\n",
      "\n",
      "avg / total       0.99      0.99      0.99      1393\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">predicted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>ham</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">actual</th>\n",
       "      <th>ham</th>\n",
       "      <td>1209</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>17</td>\n",
       "      <td>167</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            predicted     \n",
       "                  ham spam\n",
       "actual ham       1209    0\n",
       "       spam        17  167"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print confusion matrix and classification report\n",
    "print(classification_report(test_labels, predictions))\n",
    "\n",
    "pd.DataFrame(confusion_matrix(test_labels, predictions), \n",
    "             index=[['actual', 'actual'], ['ham', 'spam']],\n",
    "             columns=[['predicted', 'predicted'], ['ham', 'spam']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
